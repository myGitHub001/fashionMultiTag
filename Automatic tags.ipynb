{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15970</td>\n",
       "      <td>Men,Apparel,Topwear,Shirts,Navy Blue,Fall,Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39386</td>\n",
       "      <td>Men,Apparel,Bottomwear,Jeans,Blue,Summer,Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>59263</td>\n",
       "      <td>Women,Accessories,Watches,Watches,Silver,Winte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>21379</td>\n",
       "      <td>Men,Apparel,Bottomwear,Track Pants,Black,Fall,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>53759</td>\n",
       "      <td>Men,Apparel,Topwear,Tshirts,Grey,Summer,Casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                               tags\n",
       "0           0  15970   Men,Apparel,Topwear,Shirts,Navy Blue,Fall,Casual\n",
       "1           1  39386    Men,Apparel,Bottomwear,Jeans,Blue,Summer,Casual\n",
       "2           2  59263  Women,Accessories,Watches,Watches,Silver,Winte...\n",
       "3           3  21379  Men,Apparel,Bottomwear,Track Pants,Black,Fall,...\n",
       "4           4  53759     Men,Apparel,Topwear,Tshirts,Grey,Summer,Casual"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "styles_multiTags = pd.read_csv(\"../input/multitags/styles_multiTags.csv\")\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 39410]\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 12347]\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 39041]\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 39403]\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 39401]\n",
    "styles_multiTags = styles_multiTags[styles_multiTags.id != 39425]\n",
    "styles_multiTags43500 = styles_multiTags[:43500]\n",
    "styles_multiTags43500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles_multiTags.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir('/kaggle/input/fashion-product-images-dataset/fashion-dataset/images')\n",
    "for value in styles_multiTags['id'].values:\n",
    "    img = str(value)+\".jpg\"\n",
    "    if img not in filenames:\n",
    "        print(value)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "path = Path('../input/')\n",
    "data = ImageList.from_csv(path, 'multitags/styles_multiTags.csv', cols=1, folder='fashion-product-images-dataset/fashion-dataset/images/', suffix='.jpg')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = Path('../input/fashion-product-images-dataset/fashion-dataset/images/')\n",
    "data1 = ImageList.from_df(styles_multiTags43500, path1, cols=1, suffix='.jpg')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.split_by_rand_pct(0.1).label_from_df(cols=2, label_delim=',').transform(get_transforms(), size=224, resize_method=3).databunch(bs=64).normalize(imagenet_stats)\n",
    "#data1 = data1.split_by_rand_pct(0.1).label_from_df(cols=2, label_delim=',').transform(get_transforms(), size=512, resize_method=3).databunch(bs=64).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.train_ds.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.train_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.show_batch(rows=3, figsize=(20,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEFORE THIS STEP, Click on Add Data and Search for Resnet152 from Kaggle. Because Kaggle serves read-only dirs, we cannot download pre-trained weights for Resnet. Now we have to make a dir \n",
    "#for copying those weights and hence we are making ~/.torch/models folder. \n",
    "cache_dir = os.path.expanduser(os.path.join('/root/.cache/','torch/checkpoints'))\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir) #first make ~/.torch if not already available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = os.path.join(cache_dir,'models')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir) #then make ~/.torch/models, if not already available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied resnet34.pth, which are pretrained weights on Resnet34 to our folder into resnet<version>-<sha-hash>.pth\n",
    "!cp ../input/resnet152/resnet152-b121ed2d.pth /root/.cache/torch/checkpoints/resnet152-b121ed2d.pth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_PATH is declared this way and glued to model_dir attr of cnn_learner.\n",
    "#MODEL_PATH = '/tmp/models'\n",
    "#model_dir=MODEL_PATH\n",
    "acc_02 = partial(accuracy_thresh, thresh=0.2)\n",
    "f_score = partial(fbeta, thresh=0.2)\n",
    "learn = cnn_learner(data1, models.resnet152, metrics=[acc_02,f_score])\n",
    "learn.model_dir = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(rows=2, figsize=(24,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles_multiTags.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image('../input/fashion-product-images-dataset/fashion-dataset/images/34411.jpg')\n",
    "img = img.apply_tfms(tfms=get_transforms()[1], size=224, resize_method=3) # resize the images the same way a the pre-processing\n",
    "preds, idx, output = learn.predict(img)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict({learn.data.classes[i]: round(to_np(p)*100,2) for i, p in enumerate(output) if p > 0.2})\n",
    "img.show(title=str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, max_lr=slice(1e-06,1e-04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE_SIZE = [512, 512]\n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "STEPS_PER_EPOCH = len(data1.train_ds)// BATCH_SIZE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with strategy.scope():\n",
    "    #pretrained_model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
    "    #pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
    "    pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
    "    pretrained_model.trainable = False # False = transfer learning, True = fine-tuning\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(len(data1.classes), activation='softmax')\n",
    "    ])\n",
    "        \n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(data1.train_ds.x, data1.train_ds.y, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=(data1.valid_ds.x, data1.valid_ds.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
